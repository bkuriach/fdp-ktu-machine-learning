{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2c6c718-0d53-4d07-9a78-b27f931d407f",
   "metadata": {},
   "source": [
    "## Implement Neural Network using PyTorch\n",
    "* Load Data\n",
    "* Define PyToch Model\n",
    "* Define Loss Function and Optimizers\n",
    "* Run a Training Loop\n",
    "* Evaluate the Model\n",
    "* Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "950a0db2-01fe-4a0e-9cc6-8d1c07036b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dda46185-e102-4945-898f-e0c44a877d44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e29ffb3-dd82-41c7-bbc4-f9a2c4635ad5",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "Pima Indians onset of diabetes dataset. This has been a standard machine learning dataset since the early days of the field. It describes patient medical record data for Pima Indians and whether they had an onset of diabetes within five years.\n",
    "\n",
    "**Input Variables**\n",
    "* Number of times pregnant\n",
    "* Plasma glucose concentration at 2 hours in an oral glucose tolerance test\n",
    "* Diastolic blood pressure (mm Hg)\n",
    "* Triceps skin fold thickness (mm)\n",
    "* 2-hour serum insulin (μIU/ml)\n",
    "* Body mass index (weight in kg/(height in m)2)\n",
    "* Diabetes pedigree function\n",
    "* Age (years)\n",
    "\n",
    "**Output Variables**\n",
    "\n",
    "* Class label (0 or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e79df18-5180-4f30-9b7d-1372331237cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# load the dataset, split into input (X) and output (y) variables\n",
    "dataset = np.loadtxt('./data/pima-indians-diabetes.csv', delimiter=',')\n",
    "X = dataset[:,0:8]\n",
    "y = dataset[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9dbfda8-ed05-4dc8-b897-264ac1a72baa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf001aa-9d70-4665-8e35-6e75cc82b5c6",
   "metadata": {},
   "source": [
    "## Define Model\n",
    "A model can be defined as a sequence of layers. You create a Sequential model with the layers listed out. The first thing you need to do to get this right is to ensure the first layer has the correct number of input features. In this example, you can specify the input dimension  8 for the eight input variables as one vector.\n",
    "\n",
    "* The model expects rows of data with 8 variables (the first argument at the first layer set to 8)\n",
    "* The first hidden layer has 12 neurons, followed by a ReLU activation function\n",
    "* The second hidden layer has 8 neurons, followed by another ReLU activation function\n",
    "* The output layer has one neuron, followed by a sigmoid activation function\n",
    "\n",
    "In this approach, a class needs to have all the layers defined in the constructor because you need to prepare all its components when it is created, but the input is not yet provided. Note that you also need to call the parent class’s constructor (the line super().__init__()) to bootstrap your model. You also need to define a forward() function in the class to tell, if an input tensor x is provided, how you produce the output tensor in return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7079483-f765-496b-ab56-e99c19b5a1a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PimaClassifier(\n",
      "  (hidden1): Linear(in_features=8, out_features=12, bias=True)\n",
      "  (act1): ReLU()\n",
      "  (hidden2): Linear(in_features=12, out_features=8, bias=True)\n",
      "  (act2): ReLU()\n",
      "  (output): Linear(in_features=8, out_features=1, bias=True)\n",
      "  (act_output): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class PimaClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(8, 12)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.hidden2 = nn.Linear(12, 8)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.output = nn.Linear(8, 1)\n",
    "        self.act_output = nn.Sigmoid()\n",
    " \n",
    "    def forward(self, x):\n",
    "        x= self.hidden1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.act2(self.hidden2(x))\n",
    "        x = self.act_output(self.output(x))\n",
    "        return x\n",
    " \n",
    "model = PimaClassifier()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7845ac78-fa4c-4344-9f31-c7ba8de6ab81",
   "metadata": {},
   "source": [
    "## Define Loss Function and Optimizer\n",
    "\n",
    "We need neural network model to predict the results as close as the actual. Training the Neural Network model means finding the best set of weights to map the input to outputs in your dataset. The loss or cost function us the metric to measure the difference between prediction and actuals. In this problem we are using binary cross entropy because it is a binary classification problem.\n",
    "\n",
    "After the loss function, we need an optimizer. Optimizer is the algorithm used to adjust the model weights and theirby minimize the cost function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9904839b-87a7-4188-8177-1385ab09058c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()  # binary cross entropy\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a448c3f-a335-4d89-be77-2079f5f94f8b",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Training a neural network model usually takes in epochs and batches. They are idioms for how data is passed to a model:\n",
    "\n",
    "* **Epoch**: Passes the entire training dataset to the model once\n",
    "* **Batch**: One or more samples passed to the model, from which the gradient descent algorithm will be executed for one iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e21cc1d-b720-49a7-8b12-5aa31d7647ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋           | 6/100 [00:00<00:01, 55.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 0, latest loss 0.4929962754249573\n",
      "Finished epoch 10, latest loss 0.4554964303970337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███▎       | 30/100 [00:00<00:00, 70.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 20, latest loss 0.44788020849227905\n",
      "Finished epoch 30, latest loss 0.44748398661613464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▉     | 54/100 [00:00<00:00, 72.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 40, latest loss 0.4265342652797699\n",
      "Finished epoch 50, latest loss 0.4158594608306885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████▋   | 70/100 [00:00<00:00, 71.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 60, latest loss 0.39098966121673584\n",
      "Finished epoch 70, latest loss 0.3696490526199341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|██████████▎| 94/100 [00:01<00:00, 70.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 80, latest loss 0.35486742854118347\n",
      "Finished epoch 90, latest loss 0.34297993779182434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 70.30it/s]\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "batch_size = 10\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        Xbatch = X[i:i+batch_size]\n",
    "        y_pred = model(Xbatch)\n",
    "        ybatch = y[i:i+batch_size]\n",
    "        loss = loss_fn(y_pred, ybatch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch%10 == 0:\n",
    "        print(f'Finished epoch {epoch}, latest loss {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028d88e6-e064-4b45-9605-80a09936da31",
   "metadata": {},
   "source": [
    "## Evalaute the Model\n",
    "\n",
    "You have trained our neural network on the entire dataset, and you can evaluate the performance of the network on the same dataset. This will only give you an idea of how well you have modeled the dataset (e.g., train accuracy) but no idea of how well the algorithm might perform on new data. This was done for simplicity, but ideally, you could separate your data into train and test datasets for training and evaluation of your model.\n",
    "\n",
    "You can evaluate your model on your training dataset in the same way you invoked the model in training. This will generate predictions for each input, but then you still need to compute a score for the evaluation. This score can be the same as your loss function or something different. Because you are doing binary classification, you can use accuracy as your evaluation score by converting the output (a floating point in the range of 0 to 1) to an integer (0 or 1) and compare to the label we know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c7891e3-7f74-450d-821e-ceb052e55111",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.7864583134651184\n"
     ]
    }
   ],
   "source": [
    "# compute accuracy (no_grad is optional)\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X)\n",
    " \n",
    "accuracy = (y_pred.round() == y).float().mean()\n",
    "print(f\"Accuracy {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21afaba-a610-41e9-a3b3-462df47d9809",
   "metadata": {},
   "source": [
    "## Make class predictions with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b8514c7-2d52-48af-b2c6-b8a10c696064",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.0, 148.0, 72.0, 35.0, 0.0, 33.599998474121094, 0.6269999742507935, 50.0] => 1 (expected 1)\n",
      "[1.0, 85.0, 66.0, 29.0, 0.0, 26.600000381469727, 0.35100001096725464, 31.0] => 0 (expected 0)\n",
      "[8.0, 183.0, 64.0, 0.0, 0.0, 23.299999237060547, 0.671999990940094, 32.0] => 1 (expected 1)\n",
      "[1.0, 89.0, 66.0, 23.0, 94.0, 28.100000381469727, 0.16699999570846558, 21.0] => 0 (expected 0)\n",
      "[0.0, 137.0, 40.0, 35.0, 168.0, 43.099998474121094, 2.2880001068115234, 33.0] => 1 (expected 1)\n",
      "[5.0, 116.0, 74.0, 0.0, 0.0, 25.600000381469727, 0.20100000500679016, 30.0] => 0 (expected 0)\n",
      "[3.0, 78.0, 50.0, 32.0, 88.0, 31.0, 0.24799999594688416, 26.0] => 0 (expected 1)\n",
      "[10.0, 115.0, 0.0, 0.0, 0.0, 35.29999923706055, 0.1340000033378601, 29.0] => 1 (expected 0)\n",
      "[2.0, 197.0, 70.0, 45.0, 543.0, 30.5, 0.15800000727176666, 53.0] => 1 (expected 1)\n",
      "[8.0, 125.0, 96.0, 0.0, 0.0, 0.0, 0.23199999332427979, 54.0] => 0 (expected 1)\n",
      "[4.0, 110.0, 92.0, 0.0, 0.0, 37.599998474121094, 0.19099999964237213, 30.0] => 0 (expected 0)\n",
      "[10.0, 168.0, 74.0, 0.0, 0.0, 38.0, 0.5370000004768372, 34.0] => 1 (expected 1)\n",
      "[10.0, 139.0, 80.0, 0.0, 0.0, 27.100000381469727, 1.440999984741211, 57.0] => 0 (expected 0)\n",
      "[1.0, 189.0, 60.0, 23.0, 846.0, 30.100000381469727, 0.39800000190734863, 59.0] => 1 (expected 1)\n",
      "[5.0, 166.0, 72.0, 19.0, 175.0, 25.799999237060547, 0.5870000123977661, 51.0] => 1 (expected 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "predictions = (model(X) > 0.5).int()\n",
    "for i in range(15):\n",
    "    print('%s => %d (expected %d)' % (X[i].tolist(), predictions[i], y[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a773df-8bf8-4f84-8253-f595d9d04879",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
