{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LENGTH = 200\n",
    "SPLIT = 0.8\n",
    "BATCH_SIZE = 50\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "EMBEDDING_DIM = 150\n",
    "HIDDEN_DIM = 256\n",
    "N_LAYERS = 2\n",
    "OUTPUT_SIZE = 1\n",
    "EPOCHS = 1\n",
    "PRINT_EVERY = 100\n",
    "CLIP=5\n",
    "MODEL_ARCH='LSTM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset:\n",
    "    def __init__(self):\n",
    "        self.review = None\n",
    "        self.labels = None\n",
    "        self.reviews_split = None\n",
    "        self.vocab_to_int = None\n",
    "        self.int_to_vocab = None\n",
    "        self.review_ints = None\n",
    "        self.encoded_labels = None\n",
    "        self.features = None\n",
    "\n",
    "    def load_data(self):\n",
    "        with open('D:/Projects/Sessions/TextClassification/DeepLearning/input/reviews.txt', 'r') as f:\n",
    "            self.reviews = f.read()\n",
    "        with open('/TextClassification/DeepLearning/input/labels.txt', 'r') as f:\n",
    "            self.labels = f.read()\n",
    "\n",
    "    def clean_data(self):\n",
    "        self.reviews = self.reviews.lower()\n",
    "        self.all_text = ''.join([c for c in self.reviews if c not in punctuation])\n",
    "        self.reviews_split = self.all_text.split('\\n')\n",
    "        self.all_text = ' '.join(self.reviews_split)\n",
    "        self.words = self.all_text.split()\n",
    "        # return self.reviews_split\n",
    "\n",
    "    def vocab_dict(self):\n",
    "        counts = Counter(self.words)\n",
    "        vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "        self.vocab_to_int = {word: ii for ii, word in enumerate(vocab)}\n",
    "        self.int_to_vocab = {ii: word for word,ii in self.vocab_to_int.items() }\n",
    "        # return self.vocab_to_int, self.int_to_vocab\n",
    "\n",
    "    def encode_text(self):\n",
    "        self.review_ints = []\n",
    "        for reviews in self.reviews_split:\n",
    "            self.review_ints.append([self.vocab_to_int[word] for word in reviews.split()])\n",
    "        # return self.review_ints\n",
    "\n",
    "    def encode_label(self):\n",
    "        self.labels_split = self.labels.split('\\n')\n",
    "        self.encoded_labels = np.array([1 if label == 'positive' else 0 for label in self.labels_split])\n",
    "        # return self.encoded_labels\n",
    "\n",
    "    def remove_outliers(self):\n",
    "        review_lens = Counter([len(x) for x in self.review_ints])\n",
    "        print(\"Zero-length reviews: {}\".format(review_lens[0]))\n",
    "        print(\"Maximum review length: {}\".format(max(review_lens)))\n",
    "        print('Number of reviews before removing outliers: ', len(self.review_ints))\n",
    "        non_zero_idx = [ii for ii, review in enumerate(self.review_ints) if len(review) != 0]\n",
    "        self.review_ints = [self.review_ints[ii] for ii in non_zero_idx]\n",
    "        self.encoded_labels = np.array([self.encoded_labels[ii] for ii in non_zero_idx])\n",
    "        print('Number of reviews after removing outliers: ', len(self.review_ints))\n",
    "\n",
    "        # return self.review_ints, self.encoded_labels\n",
    "\n",
    "    def pad_features(self, seq_length):\n",
    "        self.features = np.zeros((len(self.review_ints), config.SEQ_LENGTH), dtype=int)\n",
    "        for i, row in enumerate(self.review_ints):\n",
    "            self.features[i, -len(row):] = np.array(row)[:config.SEQ_LENGTH]\n",
    "\n",
    "    def process_new_instance(self, input):\n",
    "        input = str(input).lower()\n",
    "        input =''.join([x for x in input if x not in punctuation])\n",
    "        encoded_input =[]\n",
    "        for token in input.split():\n",
    "            encoded_input.append(self.vocab_to_int[token])\n",
    "        padded_input = np.zeros((1, config.SEQ_LENGTH), dtype=int)\n",
    "        padded_input[0, -len(encoded_input):] = np.array(encoded_input)[0:config.SEQ_LENGTH]\n",
    "        return padded_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(features, encoded_labels):\n",
    "    split_idx = int(len(features) * config.SPLIT)\n",
    "    train_x, remaining_x = features[:split_idx], features[split_idx:]\n",
    "    train_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n",
    "\n",
    "    test_idx = int(len(remaining_x) * 0.5)\n",
    "    val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
    "    val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n",
    "\n",
    "    ## print out the shapes of your resultant feature data\n",
    "    print(\"\\t\\t\\tFeature Shapes:\")\n",
    "    print(\"Train set: \\t\\t{}\".format(train_x.shape),\n",
    "          \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "          \"\\nTest set: \\t\\t{}\".format(test_x.shape))\n",
    "\n",
    "    return train_x, train_y, val_x, val_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_data = SentimentDataset()\n",
    "sent_data.load_data()\n",
    "sent_data.clean_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_data.vocab_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_data.encode_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_data.encode_label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-length reviews: 1\n",
      "Maximum review length: 2514\n",
      "Number of reviews before removing outliers:  25001\n",
      "Number of reviews after removing outliers:  25000\n"
     ]
    }
   ],
   "source": [
    "sent_data.remove_outliers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_data.pad_features(SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-length reviews: 1\n",
      "Maximum review length: 2514\n",
      "Number of reviews before removing outliers:  25001\n",
      "Number of reviews after removing outliers:  25000\n"
     ]
    }
   ],
   "source": [
    "# sent_data = SentimentDataset()\n",
    "# sent_data.load_data()\n",
    "# sent_data.clean_data()\n",
    "# sent_data.vocab_dict()\n",
    "# sent_data.encode_text()\n",
    "# sent_data.encode_label()\n",
    "# sent_data.remove_outliers()\n",
    "# sent_data.pad_features(SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(20000, 200) \n",
      "Validation set: \t(2500, 200) \n",
      "Test set: \t\t(2500, 200)\n"
     ]
    }
   ],
   "source": [
    "features= sent_data.features\n",
    "encoded_labels = sent_data.encoded_labels\n",
    "train_x, train_y, val_x, val_y, test_x, test_y = data_split(features, encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the SHUFFLE your training data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=BATCH_SIZE)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=BATCH_SIZE)\n",
    "\n",
    "vocab_size = len(sent_data.vocab_to_int)+1\n",
    "output_size = OUTPUT_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "\n",
    "        super(SentimentLSTM, self).__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers,\n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "        # embeddings and lstm_out\n",
    "        x = x.long()\n",
    "        # initial hidden states\n",
    "        h0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(config.DEVICE)\n",
    "        c0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(config.DEVICE)\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, (h0,c0))\n",
    "\n",
    "        # # stack up lstm outputs\n",
    "        # lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        lstm_out, _ = torch.max(lstm_out,1)\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        sig_out = self.sig(out)\n",
    "\n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1]  # get last batch of labels\n",
    "\n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SentimentLSTM(vocab_size, output_size, EMBEDDING_DIM, HIDDEN_DIM, N_LAYERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentLSTM(\n",
       "  (embedding): Embedding(74073, 150)\n",
       "  (lstm): LSTM(150, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (sig): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentLSTM(\n",
       "  (embedding): Embedding(74073, 150)\n",
       "  (lstm): LSTM(150, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (sig): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr=0.001\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "net.to(device=DEVICE)\n",
    "net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(data_loader, valid_loader, model, optimizer, criterion, device, model_name=\"lstm\"):\n",
    "    counter = 0\n",
    "    for e in range(config.EPOCHS):\n",
    "        for inputs, labels in data_loader:\n",
    "            counter += 1\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            model.zero_grad()\n",
    "            inputs = inputs.long()\n",
    "            output, h = model(inputs)\n",
    "            \n",
    "            loss = criterion(output.squeeze(), labels.float())\n",
    "            loss.backward()\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "            optimizer.step()\n",
    "\n",
    "            # loss stats\n",
    "            if counter % config.PRINT_EVERY == 0:\n",
    "                # Get validation loss\n",
    "                val_losses = []\n",
    "                model.eval()\n",
    "                for inputs, labels in valid_loader:\n",
    "                    inputs, labels = inputs.to(config.DEVICE), labels.to(config.DEVICE)\n",
    "                    # output, val_h = model(inputs)\n",
    "                    output, val_h = model(inputs)\n",
    "                    # output = model(inputs)\n",
    "                    val_loss = criterion(output.squeeze(), labels.float())\n",
    "                    val_losses.append(val_loss.item())\n",
    "\n",
    "                model.train()\n",
    "                print(\"Epoch: {}/{}...\".format(e+1, config.EPOCHS),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1... Step: 100... Loss: 0.634634... Val Loss: 0.620509\n",
      "Epoch: 1/1... Step: 200... Loss: 0.507708... Val Loss: 0.538738\n",
      "Epoch: 1/1... Step: 300... Loss: 0.492999... Val Loss: 0.507180\n",
      "Epoch: 1/1... Step: 400... Loss: 0.378106... Val Loss: 0.438782\n"
     ]
    }
   ],
   "source": [
    "net = train_fn(train_loader, valid_loader, net, optimizer, criterion, config.DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_fn(test_loader, model, criterion, device):\n",
    "    # Get test data loss and accuracy\n",
    "    test_losses = []  # track loss\n",
    "    num_correct = 0\n",
    "    model.eval()\n",
    "    # iterate over test data\n",
    "    for inputs, labels in test_loader:\n",
    "\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # get predicted outputs\n",
    "\n",
    "        # output, h = model(inputs)\n",
    "        # output = model(inputs)\n",
    "        if config.MODEL_ARCH == 'CNN':\n",
    "            output = model(inputs)\n",
    "        else:\n",
    "            output, h = model(inputs)\n",
    "\n",
    "        # calculate loss\n",
    "        test_loss = criterion(output.squeeze(), labels.float())\n",
    "        test_losses.append(test_loss.item())\n",
    "\n",
    "        # convert output probabilities to predicted class (0 or 1)\n",
    "        pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
    "\n",
    "        # compare predictions to true label\n",
    "        correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "        correct = np.squeeze(correct_tensor.cpu().numpy()) #if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "        num_correct += np.sum(correct)\n",
    "\n",
    "    # -- stats! -- ##\n",
    "    # avg test loss\n",
    "    print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "\n",
    "    # accuracy over all test data\n",
    "    test_acc = num_correct / len(test_loader.dataset)\n",
    "    print(\"Test accuracy: {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.444\n",
      "Test accuracy: 0.794\n"
     ]
    }
   ],
   "source": [
    "test_fn(test_loader, net, criterion, config.DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[9, 114,10,  17, -1]\n",
    "[0....0,9, 114,10,  17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_data.vocab_to_int.get('dkjnqdfkjvn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   9, 114,  10,  17]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_data.process_new_instance(str(\"I love this movie abcd\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data_object, input):\n",
    "\n",
    "    padded_input = data_object.process_new_instance(str(input))\n",
    "    padded_input = torch.from_numpy(padded_input)\n",
    "    padded_input = padded_input.to(config.DEVICE)\n",
    "\n",
    "    if config.MODEL_ARCH == 'CNN':\n",
    "        output = model(padded_input)\n",
    "    else:\n",
    "        output, h = model(padded_input)\n",
    "    # output, h = model(padded_input)\n",
    "    # output= model(padded_input)\n",
    "    confidence = output.squeeze()\n",
    "    pred = torch.round(confidence)\n",
    "    result = None\n",
    "    if pred == 1:\n",
    "        result = 'Positive'\n",
    "    else:\n",
    "        result = 'Negative'\n",
    "        confidence = 1-confidence\n",
    "\n",
    "    print(f' Review - {input} ::{result} --> confidence :{confidence}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Testing few insances \n",
      " Review -  I Love this movie ::Positive --> confidence :0.7799832820892334\n",
      " Review -  This movie is not good ::Positive --> confidence :0.5787915587425232\n",
      " Review - The worst movie I have seen; acting was terrible and I want my money back ::Negative --> confidence :0.9721530675888062\n",
      " Review -  I enjoyed this movie ::Positive --> confidence :0.7330417037010193\n",
      " Review -  this movie is pathetic ::Positive --> confidence :0.5494117140769958\n"
     ]
    }
   ],
   "source": [
    "print(\" Testing few insances \")\n",
    "engine.predict(net, sent_data ,\" I Love this movie\")\n",
    "engine.predict(net, sent_data, \" This movie is not good\")\n",
    "engine.predict(net, sent_data, \"The worst movie I have seen; acting was terrible and I want my money back\")\n",
    "engine.predict(net, sent_data, \" I enjoyed this movie\")\n",
    "engine.predict(net, sent_data, \" this movie is pathetic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
